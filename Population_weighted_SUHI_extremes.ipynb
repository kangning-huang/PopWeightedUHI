{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhJsSeoGwsw5rvzsgBBm/u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kangning-huang/PopWeightedUHI/blob/main/Population_weighted_SUHI_extremes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R4XdmmGleUZB"
      },
      "outputs": [],
      "source": [
        "# download geemap (https://geemap.org/)\n",
        "!pip install geemap &> /dev/null\n",
        "# download geopandas (https://geopandas.org/)\n",
        "!pip install geopandas &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import geopandas as gdp\n",
        "import geemap\n",
        "from geemap import geojson_to_ee, ee_to_geojson"
      ],
      "metadata": {
        "id": "VUlDg6BFe2BU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A Google Earth Engine account is required to authenticate geemap.\n",
        "# Register here https://earthengine.google.com/\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0-tqqlxe7HO",
        "outputId": "ec88146a-9233-4ee5-e302-395125c3c6b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=wirkyxHqf6CJdAXQHDgVGqGXAAcElrdv4QmjRKiNjGc&tc=_arJKc-C6tz9gtevbvNQaNDP47fabftA3igWw3ZEN9I&cc=clTr8JvXiY5dlO6GlMUhsqsuWfx2h4NZWYNglFMQQFI\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below.\n",
            "Enter verification code: 4/1Adeu5BUU0nPfG-e5zF5gpJiYv1vfFMBlkBnVRFCWTHRPAS98c2AFJfqKfyw\n",
            "\n",
            "Successfully saved authorization token.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the functional urban areas from Gobal Human Settlement Layer.\n",
        "!curl https://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/GHSL//GHS_FUA_UCDB2015_GLOBE_R2019A/V1-0/GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.zip --output GHS_FUA.zip\n",
        "!unzip GHS_FUA.zip\n",
        "gdf_FUAs = gdp.read_file('/content/GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.gpkg')\n",
        "gdf_FUAs_sel  = gdf_FUAs.sort_values('FUA_p_2015', ascending=False).head(1100)\n",
        "# gdf_FUAs_sel.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3wgXmMDfoPc",
        "outputId": "f2bfae87-5955-424f-be0c-d169f380cc54"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2987k  100 2987k    0     0  2246k      0  0:00:01  0:00:01 --:--:-- 2246k\n",
            "Archive:  GHS_FUA.zip\n",
            "replace GHSL_FUA_2019.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: GHSL_FUA_2019.pdf       \n",
            "  inflating: GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.gpkg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get UHI stats by year\n",
        "def GetUhiStatsByYear(year):\n",
        "  # getting the image collection\n",
        "  uhiModis1 = ee.ImageCollection('users/lorenzomentaschi/uhiModis');\n",
        "  uhiModis2 = ee.ImageCollection('users/lucfeyen3/uhiModis');\n",
        "  uhiModis = uhiModis1.merge(uhiModis2);\n",
        "  # uhiModis3 = ee.ImageCollection('users/lucfeyen3/uhiModisStats_Shanghai');\n",
        "  # uhiModis = uhiModis1.merge(uhiModis2).merge(uhiModis3);\n",
        "  # Filter images in the year\n",
        "  filterYear = ee.Filter.calendarRange(year, year, 'year')\n",
        "  # Filter pixles with more than 50 rual pixels\n",
        "  def FilterByMinNRuralPixel(img):\n",
        "    minNRuralPixel = 50\n",
        "    ruralPxlCnt = img.select('N_NOTURB_PIX_Day')\n",
        "    msk = ruralPxlCnt.gte(minNRuralPixel)\n",
        "    return img.select(['UHI_Day_1km', 'UHI_Night_1km']).updateMask(msk)\n",
        "  # Apply both filters\n",
        "  ImColYear = ee.ImageCollection(uhiModis.filter(filterYear).map(FilterByMinNRuralPixel))\n",
        "  date = ImColYear.first().get('system:time_start')\n",
        "  ImStatsYear = ImColYear.reduce(ee.Reducer.percentile([99]))\n",
        "  return ImStatsYear.set({'system:time_start': date})\n",
        "\n",
        "# Function to get Landscan population by year\n",
        "def GetLandscanPopByYear(year):\n",
        "  ImColPop = ee.ImageCollection(\"projects/sat-io/open-datasets/ORNL/LANDSCAN_GLOBAL\")\n",
        "  ImPopYear = ImColPop.filter(ee.Filter.calendarRange(year,year,'year')).first()\n",
        "  return ImPopYear\n",
        "\n",
        "# Function to get POP times UHI\n",
        "def GetUhiStatsTimesPopByYear(year):\n",
        "  ImUhiStat = GetUhiStatsByYear(year)\n",
        "  ImPop = GetLandscanPopByYear(year)\n",
        "  return ImUhiStat.multiply(ImPop)\n",
        "\n",
        "# Get global warm season mean from an image collection\n",
        "def GetWarmSeasonMean(imgCol, year):\n",
        "  # Filter by year\n",
        "  imgCol = imgCol.filter(ee.Filter.calendarRange(year, year, 'year'))\n",
        "  # Masks for Nothern (NH) and Southern (SN) Hemispheres\n",
        "  maskNH = ee.Image.pixelLonLat().select('latitude').gt(0)\n",
        "  maskSH = ee.Image.pixelLonLat().select('latitude').lt(0);\n",
        "  # Masks for climate zones\n",
        "  maskTemperate = ee.Image.pixelLonLat().select('latitude').abs().gt(35)\n",
        "  maskSubtropic = ee.Image.pixelLonLat().select('latitude').abs().lt(35).And(\n",
        "                  ee.Image.pixelLonLat().select('latitude').abs().gt(10))\n",
        "  maskTropical  = ee.Image.pixelLonLat().select('latitude').abs().lt(10)\n",
        "  # Filters for months in warm seasons\n",
        "  filterJJA = ee.Filter.calendarRange(6, 8, 'month')\n",
        "  filterDJF = ee.Filter.Or(\n",
        "      ee.Filter.calendarRange( 1,  2, 'month'),\n",
        "      ee.Filter.calendarRange(12, 12, 'month'))\n",
        "  filterAMJJAS = ee.Filter.calendarRange(4, 9, 'month')\n",
        "  filterONDJFM = ee.Filter.Or(\n",
        "      ee.Filter.calendarRange(10, 12, 'month'),\n",
        "      ee.Filter.calendarRange( 1,  3, 'month'))\n",
        "  # Filter images of warm seasons in different (climates + hemispheres)\n",
        "  imgColTemperateN = imgCol.filter(filterJJA)\n",
        "  imgColTemperateS = imgCol.filter(filterDJF)\n",
        "  imgColSubtropicN = imgCol.filter(filterAMJJAS)\n",
        "  imgColSubtropicS = imgCol.filter(filterONDJFM)\n",
        "  # Calculate the masked means\n",
        "  imgMeanTemperateN = imgColTemperateN.mean().updateMask(maskTemperate.And(maskNH))\n",
        "  imgMeanTemperateS = imgColTemperateS.mean().updateMask(maskTemperate.And(maskSH))\n",
        "  imgMeanSubtropicN = imgColSubtropicN.mean().updateMask(maskSubtropic.And(maskNH))\n",
        "  imgMeanSubtropicS = imgColSubtropicS.mean().updateMask(maskSubtropic.And(maskSH))\n",
        "  imgMeanTropical   = imgCol.mean().updateMask(maskTropical)\n",
        "  # Return the mosaiced image\n",
        "  return ee.ImageCollection.fromImages([imgMeanTemperateN, imgMeanTemperateS,\n",
        "                                        imgMeanSubtropicN, imgMeanSubtropicS,\n",
        "                                        imgMeanTropical]).mosaic()\n",
        "\n",
        "\n",
        "\n",
        "# Define function to calculate UHI per cap for FUA, in a year\n",
        "def GetUhiPercapFUAbyYear(gdf_fua, year, urbanMasked = False):\n",
        "  fc_fua = geemap.geopandas_to_ee(gdf_fua).first()\n",
        "  geo_fua = fc_fua.geometry()\n",
        "  uhi_cnt = GetUhiStatsByYear(year).reduceRegion(ee.Reducer.count(), geo_fua, 1000).getInfo()\n",
        "  if uhi_cnt['UHI_Day_1km_p99'] > 0:\n",
        "    # ID and name of the FUA\n",
        "    fua_id   = gdf_fua[['eFUA_ID']].iloc[0].iloc[0]\n",
        "    fua_name = gdf_fua[['eFUA_name']].iloc[0].iloc[0]\n",
        "    # Get Landscan population image for the year\n",
        "    ImPopYear = GetLandscanPopByYear(year)\n",
        "    # Area-area SUHI\n",
        "    uhi_mean = GetUhiStatsByYear(year).reduceRegion(ee.Reducer.mean(), geo_fua, 1000).getInfo()\n",
        "    # Total population\n",
        "    pop_sum_1km = ImPopYear.reduceRegion(ee.Reducer.sum(), geo_fua, 1000).getInfo()\n",
        "    # Total SUHI times population\n",
        "    uhiTimesPop_sum = GetUhiStatsTimesPopByYear(year).reduceRegion(ee.Reducer.sum(), geo_fua, 1000).getInfo()\n",
        "    # Population-weighted SUHI\n",
        "    uhi_day_p99_percap = uhiTimesPop_sum['UHI_Day_1km_p99'] / pop_sum_1km['b1']\n",
        "    # Calculate population density times population\n",
        "    ImPopTimesPopDensYear = ImPopYear.multiply(ImPopYear)\n",
        "    # Calculate total population for FUA\n",
        "    pop_total = ImPopYear.reduceRegion(\n",
        "        reducer=ee.Reducer.sum(), geometry=fc_fua.geometry(),\n",
        "        scale=1000, maxPixels=1e13).getInfo()['b1']\n",
        "    # Total pop-density times population\n",
        "    pop_dens_times_pop_total = ImPopTimesPopDensYear.reduceRegion(\n",
        "        reducer=ee.Reducer.sum(), geometry=fc_fua.geometry(),\n",
        "        scale=1000, maxPixels=1e13).getInfo()['b1']\n",
        "    # Calculate the population density per capita for FUA\n",
        "    pop_dens_percap = pop_dens_times_pop_total / pop_total\n",
        "    # Get warm season Mean Albedo of the year (https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MCD43A3#bands)\n",
        "    ImAlbYear = GetWarmSeasonMean(ee.ImageCollection(\"MODIS/061/MCD43A3\"), year).select('Albedo_WSA_shortwave').multiply(0.001)\n",
        "    # Get warm season Mean EVI of the year (https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MOD13A2#bands)\n",
        "    ImEviYear = GetWarmSeasonMean(ee.ImageCollection(\"MODIS/061/MOD13A2\"), year).select('EVI').multiply(0.0001)\n",
        "    # Calculate population times Albedo\n",
        "    ImPopTimesAlbYear = ImPopYear.multiply(ImAlbYear)\n",
        "    # Calculate population times EVI\n",
        "    ImPopTimesEviYear = ImPopYear.multiply(ImEviYear)\n",
        "    # Calculate the total Albedo times population for FUA\n",
        "    alb_times_pop_total = ImPopTimesAlbYear.reduceRegion(\n",
        "        reducer=ee.Reducer.sum(), geometry=fc_fua.geometry(),\n",
        "        scale=1000, maxPixels=1e13).getInfo()['b1']\n",
        "    # Calculate the total EVI times population for FUA\n",
        "    evi_times_pop_total = ImPopTimesEviYear.reduceRegion(\n",
        "        reducer=ee.Reducer.sum(), geometry=fc_fua.geometry(),\n",
        "        scale=1000, maxPixels=1e13).getInfo()['b1']\n",
        "    # Calculate the albedo per capita for FUA\n",
        "    alb_percap = alb_times_pop_total / pop_total\n",
        "    # Calculate the EVI per capita for FUA\n",
        "    evi_percap = evi_times_pop_total / pop_total\n",
        "    # Construct a new data frame\n",
        "    df_new = pd.DataFrame({\n",
        "        'eFUA_ID': fua_id,\n",
        "        'eFUA_name': fua_name,\n",
        "        'Year': year,\n",
        "        'pop_total': int(pop_total),\n",
        "        'SUHI_A': [uhi_mean['UHI_Day_1km_p99']],\n",
        "        'SUHI_P': [uhi_day_p99_percap],\n",
        "        'pop_dens': int(pop_dens_percap),\n",
        "        'albedo': alb_percap,\n",
        "        'EVI': evi_percap\n",
        "    }, index=[str(fua_id) + '_' + str(year)])\n",
        "  # Return the population density per capita for FUA\n",
        "  return df_new"
      ],
      "metadata": {
        "id": "7ssA9eaQgPX6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename_out = 'pop_weighted_SUHI.csv'\n",
        "\n",
        "# Create empty data frames for results\n",
        "df_pop_w_SUHI = pd.DataFrame()\n",
        "\n",
        "# Test 10 FUAs\n",
        "gdf_FUAs_sel  = gdf_FUAs.sort_values('FUA_p_2015', ascending=False).head(10)\n",
        "\n",
        "# Loop through FUAs\n",
        "for fua_id in tqdm(gdf_FUAs_sel['eFUA_ID'], total=gdf_FUAs_sel.shape[0]):\n",
        "  for year in range(2003, 2021):\n",
        "    gdf_fua = gdf_FUAs.loc[gdf_FUAs['eFUA_ID']==fua_id]\n",
        "    # Add to the result data frame\n",
        "    df_pop_w_SUHI_new = GetUhiPercapFUAbyYear(gdf_fua, year)\n",
        "    if df_pop_w_SUHI_new.empty==False:\n",
        "      df_pop_w_SUHI = pd.concat([df_pop_w_SUHI, df_pop_w_SUHI_new])\n",
        "      df_pop_w_SUHI.to_csv(filename_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S_7rATRpuMP",
        "outputId": "ae843374-e298-4012-b879-17fbd6998547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 5/10 [17:28<15:01, 180.30s/it]"
          ]
        }
      ]
    }
  ]
}